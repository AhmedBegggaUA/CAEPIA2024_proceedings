Running squirrel with 16 hidden channels and 3 hops and 0.01 learning rate and 0.2 dropout and 0.0005 weight decay
Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/squirrel/out1_node_feature_label.txt
Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/squirrel/out1_graph_edges.txt
Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_0.npz
Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_1.npz
Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_2.npz
Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_3.npz
Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_4.npz
Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_5.npz
Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_6.npz
Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_7.npz
Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_8.npz
Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_9.npz
Processing...
Done!
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.71 GiB is free. Process 5808 has 178.00 MiB memory in use. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 180.00 MiB memory in use. Process 7028 has 178.00 MiB memory in use. Process 7079 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.01 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.71 GiB is free. Process 5808 has 178.00 MiB memory in use. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 180.00 MiB memory in use. Process 7028 has 178.00 MiB memory in use. Process 7079 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.01 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.69 GiB is free. Process 5808 has 178.00 MiB memory in use. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 182.00 MiB memory in use. Process 7028 has 178.00 MiB memory in use. Process 7079 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.01 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.71 GiB is free. Process 5808 has 178.00 MiB memory in use. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 182.00 MiB memory in use. Process 7028 has 180.00 MiB memory in use. Process 7079 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.01 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.71 GiB is free. Process 5808 has 178.00 MiB memory in use. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 182.00 MiB memory in use. Process 7028 has 180.00 MiB memory in use. Process 7079 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.01 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.71 GiB is free. Process 5808 has 178.00 MiB memory in use. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 182.00 MiB memory in use. Process 7028 has 180.00 MiB memory in use. Process 7079 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.01 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.70 GiB is free. Process 5808 has 178.00 MiB memory in use. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 182.00 MiB memory in use. Process 7028 has 180.00 MiB memory in use. Process 7079 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.01 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.47 GiB is free. Process 5808 has 178.00 MiB memory in use. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 182.00 MiB memory in use. Process 7028 has 180.00 MiB memory in use. Process 7079 has 180.00 MiB memory in use. Process 7959 has 244.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.03 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.22 GiB is free. Process 5808 has 178.00 MiB memory in use. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 182.00 MiB memory in use. Process 7028 has 180.00 MiB memory in use. Process 7079 has 180.00 MiB memory in use. Process 7959 has 244.00 MiB memory in use. Process 8079 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.03 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5808 has 178.00 MiB memory in use. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 182.00 MiB memory in use. Process 7028 has 180.00 MiB memory in use. Process 7079 has 180.00 MiB memory in use. Process 7959 has 244.00 MiB memory in use. Process 8079 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.03 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5808 has 178.00 MiB memory in use. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 182.00 MiB memory in use. Process 7028 has 180.00 MiB memory in use. Process 7079 has 180.00 MiB memory in use. Process 7959 has 244.00 MiB memory in use. Process 8079 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.03 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.22 GiB is free. Process 5808 has 178.00 MiB memory in use. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 182.00 MiB memory in use. Process 7028 has 180.00 MiB memory in use. Process 7079 has 180.00 MiB memory in use. Process 7959 has 244.00 MiB memory in use. Process 8079 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.03 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 5808 has 178.00 MiB memory in use. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 182.00 MiB memory in use. Process 7028 has 180.00 MiB memory in use. Process 7079 has 180.00 MiB memory in use. Process 7959 has 244.00 MiB memory in use. Process 8079 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.03 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.22 GiB is free. Process 5808 has 178.00 MiB memory in use. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 182.00 MiB memory in use. Process 7028 has 180.00 MiB memory in use. Process 7079 has 180.00 MiB memory in use. Process 7959 has 246.00 MiB memory in use. Process 8079 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.03 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.22 GiB is free. Process 5808 has 178.00 MiB memory in use. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 182.00 MiB memory in use. Process 7028 has 180.00 MiB memory in use. Process 7079 has 180.00 MiB memory in use. Process 7959 has 246.00 MiB memory in use. Process 8079 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.03 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.22 GiB is free. Process 5808 has 178.00 MiB memory in use. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 182.00 MiB memory in use. Process 7028 has 180.00 MiB memory in use. Process 7079 has 180.00 MiB memory in use. Process 7959 has 246.00 MiB memory in use. Process 8079 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.001 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.22 GiB is free. Process 5808 has 178.00 MiB memory in use. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 182.00 MiB memory in use. Process 7028 has 180.00 MiB memory in use. Process 7079 has 180.00 MiB memory in use. Process 7959 has 246.00 MiB memory in use. Process 8079 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.001 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.22 GiB is free. Process 5808 has 178.00 MiB memory in use. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 182.00 MiB memory in use. Process 7028 has 180.00 MiB memory in use. Process 7079 has 180.00 MiB memory in use. Process 7959 has 246.00 MiB memory in use. Process 8079 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.001 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.22 GiB is free. Process 5808 has 178.00 MiB memory in use. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 182.00 MiB memory in use. Process 7028 has 180.00 MiB memory in use. Process 7079 has 180.00 MiB memory in use. Process 7959 has 246.00 MiB memory in use. Process 8079 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.001 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.22 GiB is free. Process 5808 has 178.00 MiB memory in use. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 182.00 MiB memory in use. Process 7028 has 180.00 MiB memory in use. Process 7079 has 180.00 MiB memory in use. Process 7959 has 246.00 MiB memory in use. Process 8079 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.001 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 5808 has 178.00 MiB memory in use. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 184.00 MiB memory in use. Process 7028 has 180.00 MiB memory in use. Process 7079 has 180.00 MiB memory in use. Process 7959 has 246.00 MiB memory in use. Process 8079 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.001 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 5808 has 178.00 MiB memory in use. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 184.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 182.00 MiB memory in use. Process 7959 has 246.00 MiB memory in use. Process 8079 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.001 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5808 has 178.00 MiB memory in use. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 184.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 182.00 MiB memory in use. Process 7959 has 246.00 MiB memory in use. Process 8079 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.001 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 5808 has 178.00 MiB memory in use. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 184.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 182.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.003 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.23 GiB is free. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 184.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 182.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Process 9472 has 158.00 MiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.003 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 184.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 182.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 264.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.003 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 184.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 182.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 264.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.003 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 184.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 182.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 264.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.003 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 184.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 182.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 264.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.003 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 184.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 182.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 264.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.003 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 184.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 182.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 264.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 3 hops and 0.003 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 184.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 182.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 264.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.01 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 184.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 182.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 264.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.01 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 184.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 182.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 264.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.01 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 184.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 182.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 264.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.01 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 184.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 182.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 264.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.01 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 184.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 264.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.01 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 268.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.01 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 268.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.01 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5877 has 180.00 MiB memory in use. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 268.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.03 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.37 GiB is free. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 268.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.03 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 268.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.03 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 268.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.03 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 268.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.03 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 268.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.03 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 268.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.03 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 182.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 270.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.03 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 270.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.001 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 270.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.001 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 270.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.001 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 270.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.001 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 272.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.001 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 7959 has 248.00 MiB memory in use. Process 8079 has 272.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.001 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 7959 has 250.00 MiB memory in use. Process 8079 has 272.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.001 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6009 has 178.00 MiB memory in use. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 7959 has 250.00 MiB memory in use. Process 8079 has 272.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.001 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.47 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 7959 has 250.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Process 11798 has 170.00 MiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.2,1000,3,0.0005,77.2,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.003 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 7959 has 250.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.2,1000,3,0.0005,77.2,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.003 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 7959 has 250.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.2,1000,3,0.0005,77.2,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.003 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.45 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.0005,85.15,1.21
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.003 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 258.00 MiB memory in use. Process 12137 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.0005,85.15,1.21
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.003 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 258.00 MiB memory in use. Process 12137 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.0005,85.15,1.21
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.003 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 260.00 MiB memory in use. Process 12137 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.0005,85.15,1.21
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.003 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 260.00 MiB memory in use. Process 12137 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.0005,85.15,1.21
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 4 hops and 0.003 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 260.00 MiB memory in use. Process 12137 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.0005,85.15,1.21
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.01 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 260.00 MiB memory in use. Process 12137 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.0005,85.15,1.21
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.01 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 260.00 MiB memory in use. Process 12137 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.0005,85.15,1.21
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.01 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 262.00 MiB memory in use. Process 12137 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.0005,85.15,1.21
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.01 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 262.00 MiB memory in use. Process 12137 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.0005,85.15,1.21
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.01 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 262.00 MiB memory in use. Process 12137 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.0005,85.15,1.21
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.01 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 262.00 MiB memory in use. Process 12137 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.0005,85.15,1.21
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.01 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 262.00 MiB memory in use. Process 12137 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.0005,85.15,1.21
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.01 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 264.00 MiB memory in use. Process 12137 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.0005,85.15,1.21
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.03 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 264.00 MiB memory in use. Process 12137 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.0005,85.15,1.21
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.03 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 264.00 MiB memory in use. Process 12137 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.0005,85.15,1.21
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.03 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 264.00 MiB memory in use. Process 12137 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.0005,85.15,1.21
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.03 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 7079 has 184.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 264.00 MiB memory in use. Process 12137 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.0005,91.57,2.64
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.03 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 178.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 266.00 MiB memory in use. Process 12137 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Process 13439 has 178.00 MiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.0005,91.57,2.64
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.03 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 6961 has 186.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 266.00 MiB memory in use. Process 12137 has 264.00 MiB memory in use. Process 13439 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.2,1000,3,0.0005,80.54,6.02
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.03 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.22 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 266.00 MiB memory in use. Process 12137 has 264.00 MiB memory in use. Process 13439 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Process 13635 has 158.00 MiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.2,1000,3,0.0005,80.54,6.02
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.03 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 266.00 MiB memory in use. Process 12137 has 264.00 MiB memory in use. Process 13439 has 178.00 MiB memory in use. Process 13635 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.2,1000,3,0.0005,80.54,6.02
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.001 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 266.00 MiB memory in use. Process 12137 has 264.00 MiB memory in use. Process 13439 has 178.00 MiB memory in use. Process 13635 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.2,1000,3,0.0005,80.54,6.02
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.001 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 7028 has 184.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 266.00 MiB memory in use. Process 12137 has 264.00 MiB memory in use. Process 13439 has 178.00 MiB memory in use. Process 13635 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.0005,83.24,2.65
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.001 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 266.00 MiB memory in use. Process 12137 has 264.00 MiB memory in use. Process 13439 has 178.00 MiB memory in use. Process 13635 has 180.00 MiB memory in use. Process 13987 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.0005,83.24,2.65
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.001 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 266.00 MiB memory in use. Process 12137 has 264.00 MiB memory in use. Process 13439 has 178.00 MiB memory in use. Process 13635 has 180.00 MiB memory in use. Process 13987 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.0005,83.24,2.65
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.001 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 266.00 MiB memory in use. Process 12137 has 264.00 MiB memory in use. Process 13439 has 178.00 MiB memory in use. Process 13635 has 180.00 MiB memory in use. Process 13987 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.0005,83.24,2.65
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.001 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 268.00 MiB memory in use. Process 12137 has 264.00 MiB memory in use. Process 13439 has 180.00 MiB memory in use. Process 13635 has 180.00 MiB memory in use. Process 13987 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.0005,83.24,2.65
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.001 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 268.00 MiB memory in use. Process 12137 has 264.00 MiB memory in use. Process 13439 has 180.00 MiB memory in use. Process 13635 has 180.00 MiB memory in use. Process 13987 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.0005,83.24,2.65
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.001 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 268.00 MiB memory in use. Process 12137 has 264.00 MiB memory in use. Process 13439 has 180.00 MiB memory in use. Process 13635 has 182.00 MiB memory in use. Process 13987 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.0005,83.24,2.65
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.003 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 268.00 MiB memory in use. Process 12137 has 264.00 MiB memory in use. Process 13439 has 180.00 MiB memory in use. Process 13635 has 182.00 MiB memory in use. Process 13987 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.0005,83.24,2.65
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.003 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 268.00 MiB memory in use. Process 12137 has 264.00 MiB memory in use. Process 13439 has 180.00 MiB memory in use. Process 13635 has 182.00 MiB memory in use. Process 13987 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.0005,83.24,2.65
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.003 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 270.00 MiB memory in use. Process 12137 has 264.00 MiB memory in use. Process 13439 has 180.00 MiB memory in use. Process 13635 has 182.00 MiB memory in use. Process 13987 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.0005,83.24,2.65
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.003 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 270.00 MiB memory in use. Process 12137 has 264.00 MiB memory in use. Process 13439 has 180.00 MiB memory in use. Process 13635 has 182.00 MiB memory in use. Process 13987 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.0005,83.24,2.65
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.003 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 270.00 MiB memory in use. Process 12137 has 264.00 MiB memory in use. Process 13439 has 180.00 MiB memory in use. Process 13635 has 182.00 MiB memory in use. Process 13987 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.0005,83.24,2.65
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.003 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 270.00 MiB memory in use. Process 12137 has 264.00 MiB memory in use. Process 13439 has 180.00 MiB memory in use. Process 13635 has 182.00 MiB memory in use. Process 13987 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.0005,83.24,2.65
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.003 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 270.00 MiB memory in use. Process 12137 has 266.00 MiB memory in use. Process 13439 has 180.00 MiB memory in use. Process 13635 has 182.00 MiB memory in use. Process 13987 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.0005,83.24,2.65
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 5 hops and 0.003 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 274.00 MiB memory in use. Process 12137 has 266.00 MiB memory in use. Process 13439 has 180.00 MiB memory in use. Process 13635 has 182.00 MiB memory in use. Process 13987 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.0005,83.24,2.65
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.01 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 274.00 MiB memory in use. Process 12137 has 266.00 MiB memory in use. Process 13439 has 180.00 MiB memory in use. Process 13635 has 182.00 MiB memory in use. Process 13987 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.0005,83.24,2.65
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.01 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 274.00 MiB memory in use. Process 12137 has 266.00 MiB memory in use. Process 13439 has 180.00 MiB memory in use. Process 13635 has 182.00 MiB memory in use. Process 13987 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.0005,83.24,2.65
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.01 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 6655 has 406.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 274.00 MiB memory in use. Process 12137 has 266.00 MiB memory in use. Process 13439 has 180.00 MiB memory in use. Process 13635 has 182.00 MiB memory in use. Process 13987 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.0005,83.24,2.65
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.01 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.58 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 274.00 MiB memory in use. Process 12137 has 266.00 MiB memory in use. Process 13439 has 180.00 MiB memory in use. Process 13635 has 182.00 MiB memory in use. Process 13987 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.0005,83.24,2.65
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.01 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 274.00 MiB memory in use. Process 12137 has 266.00 MiB memory in use. Process 13439 has 180.00 MiB memory in use. Process 13635 has 182.00 MiB memory in use. Process 13987 has 180.00 MiB memory in use. Process 15302 has 404.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.0005,83.24,2.65
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.01 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 274.00 MiB memory in use. Process 12137 has 266.00 MiB memory in use. Process 13439 has 180.00 MiB memory in use. Process 13635 has 182.00 MiB memory in use. Process 13987 has 180.00 MiB memory in use. Process 15302 has 404.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.0005,83.24,2.65
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.01 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 274.00 MiB memory in use. Process 12137 has 266.00 MiB memory in use. Process 13439 has 180.00 MiB memory in use. Process 13635 has 182.00 MiB memory in use. Process 13987 has 180.00 MiB memory in use. Process 15302 has 404.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.0005,83.24,2.65
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.01 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 274.00 MiB memory in use. Process 12137 has 266.00 MiB memory in use. Process 13439 has 180.00 MiB memory in use. Process 13635 has 184.00 MiB memory in use. Process 13987 has 180.00 MiB memory in use. Process 15302 has 404.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.0005,83.24,2.65
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.03 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 11906 has 274.00 MiB memory in use. Process 12137 has 268.00 MiB memory in use. Process 13439 has 182.00 MiB memory in use. Process 13635 has 184.00 MiB memory in use. Process 13987 has 182.00 MiB memory in use. Process 15302 has 404.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.0005,83.24,2.65
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.03 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.45 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 12137 has 268.00 MiB memory in use. Process 13439 has 182.00 MiB memory in use. Process 13635 has 184.00 MiB memory in use. Process 13987 has 182.00 MiB memory in use. Process 15302 has 404.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.2,1000,3,0.001,77.16,1.25
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.03 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 12137 has 268.00 MiB memory in use. Process 13439 has 182.00 MiB memory in use. Process 13635 has 184.00 MiB memory in use. Process 13987 has 182.00 MiB memory in use. Process 15302 has 404.00 MiB memory in use. Process 15778 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.2,1000,3,0.001,77.16,1.25
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.03 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 6579 has 306.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 12137 has 268.00 MiB memory in use. Process 13439 has 182.00 MiB memory in use. Process 13635 has 184.00 MiB memory in use. Process 13987 has 182.00 MiB memory in use. Process 15302 has 404.00 MiB memory in use. Process 15778 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.2,1000,3,0.001,77.16,1.25
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.03 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.49 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 12137 has 268.00 MiB memory in use. Process 13439 has 182.00 MiB memory in use. Process 13635 has 184.00 MiB memory in use. Process 13987 has 182.00 MiB memory in use. Process 15302 has 404.00 MiB memory in use. Process 15778 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.2,1000,3,0.001,77.16,1.25
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.03 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.29 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 182.00 MiB memory in use. Process 13635 has 184.00 MiB memory in use. Process 13987 has 182.00 MiB memory in use. Process 15302 has 404.00 MiB memory in use. Process 15778 has 258.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Process 16158 has 176.00 MiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.03 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 182.00 MiB memory in use. Process 13635 has 184.00 MiB memory in use. Process 13987 has 182.00 MiB memory in use. Process 15302 has 404.00 MiB memory in use. Process 15778 has 260.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.03 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 182.00 MiB memory in use. Process 13635 has 184.00 MiB memory in use. Process 13987 has 182.00 MiB memory in use. Process 15302 has 404.00 MiB memory in use. Process 15778 has 260.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.001 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 182.00 MiB memory in use. Process 13635 has 184.00 MiB memory in use. Process 13987 has 182.00 MiB memory in use. Process 15302 has 404.00 MiB memory in use. Process 15778 has 260.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.001 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 182.00 MiB memory in use. Process 13635 has 184.00 MiB memory in use. Process 13987 has 182.00 MiB memory in use. Process 15302 has 404.00 MiB memory in use. Process 15778 has 260.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.001 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 182.00 MiB memory in use. Process 13635 has 184.00 MiB memory in use. Process 13987 has 182.00 MiB memory in use. Process 15302 has 404.00 MiB memory in use. Process 15778 has 260.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.001 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 182.00 MiB memory in use. Process 13635 has 184.00 MiB memory in use. Process 13987 has 182.00 MiB memory in use. Process 15302 has 404.00 MiB memory in use. Process 15778 has 260.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.001 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 182.00 MiB memory in use. Process 13635 has 184.00 MiB memory in use. Process 13987 has 182.00 MiB memory in use. Process 15302 has 404.00 MiB memory in use. Process 15778 has 262.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.001 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 182.00 MiB memory in use. Process 13635 has 184.00 MiB memory in use. Process 13987 has 182.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 262.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.001 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 182.00 MiB memory in use. Process 13635 has 184.00 MiB memory in use. Process 13987 has 182.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 262.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.001 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 182.00 MiB memory in use. Process 13635 has 184.00 MiB memory in use. Process 13987 has 182.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 262.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.003 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 182.00 MiB memory in use. Process 13635 has 184.00 MiB memory in use. Process 13987 has 182.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 262.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.003 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 182.00 MiB memory in use. Process 13635 has 184.00 MiB memory in use. Process 13987 has 182.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 264.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.003 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 184.00 MiB memory in use. Process 13987 has 182.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 264.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.003 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 184.00 MiB memory in use. Process 13987 has 182.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 264.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.003 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 184.00 MiB memory in use. Process 13987 has 182.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 264.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.003 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 184.00 MiB memory in use. Process 13987 has 182.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 266.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.003 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 184.00 MiB memory in use. Process 13987 has 182.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 266.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 16 hidden channels and 10 hops and 0.003 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 186.00 MiB memory in use. Process 13987 has 184.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 266.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.01 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 186.00 MiB memory in use. Process 13987 has 184.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 266.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.01 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 186.00 MiB memory in use. Process 13987 has 184.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 266.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.01 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 186.00 MiB memory in use. Process 13987 has 184.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 268.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.01 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 186.00 MiB memory in use. Process 13987 has 184.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 268.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.01 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 186.00 MiB memory in use. Process 13987 has 184.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 268.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.01 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 186.00 MiB memory in use. Process 13987 has 184.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 268.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.01 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 186.00 MiB memory in use. Process 13987 has 184.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 268.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.01 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 186.00 MiB memory in use. Process 13987 has 184.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 268.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.03 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 186.00 MiB memory in use. Process 13987 has 184.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 268.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.03 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 186.00 MiB memory in use. Process 13987 has 184.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 268.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.03 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 5953 has 282.00 MiB memory in use. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 186.00 MiB memory in use. Process 13987 has 184.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 268.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.03 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.46 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 188.00 MiB memory in use. Process 13987 has 184.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 270.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.03 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.28 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 188.00 MiB memory in use. Process 13987 has 184.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 270.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Process 18302 has 184.00 MiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.03 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 188.00 MiB memory in use. Process 13987 has 184.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 270.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 264.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.03 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 188.00 MiB memory in use. Process 13987 has 184.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 270.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 266.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.03 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.17 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 188.00 MiB memory in use. Process 13987 has 184.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 270.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 266.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.001 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.17 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 188.00 MiB memory in use. Process 13987 has 186.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 272.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 266.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.001 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.17 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 188.00 MiB memory in use. Process 13987 has 186.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 272.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 266.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.001 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.17 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 184.00 MiB memory in use. Process 13635 has 188.00 MiB memory in use. Process 13987 has 186.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 272.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 266.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.001 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.17 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 186.00 MiB memory in use. Process 13635 has 188.00 MiB memory in use. Process 13987 has 186.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 272.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 266.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.001 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.17 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 186.00 MiB memory in use. Process 13635 has 188.00 MiB memory in use. Process 13987 has 186.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 272.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 266.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.001 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.17 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 186.00 MiB memory in use. Process 13635 has 188.00 MiB memory in use. Process 13987 has 186.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 274.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 266.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.001 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.17 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 186.00 MiB memory in use. Process 13635 has 188.00 MiB memory in use. Process 13987 has 186.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 274.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 266.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.001 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.17 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 186.00 MiB memory in use. Process 13635 has 188.00 MiB memory in use. Process 13987 has 186.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 274.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 266.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.003 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.17 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 186.00 MiB memory in use. Process 13635 has 188.00 MiB memory in use. Process 13987 has 186.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 15778 has 274.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 268.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.2,1000,3,0.001,84.99,1.41
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.003 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.44 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 186.00 MiB memory in use. Process 13635 has 188.00 MiB memory in use. Process 13987 has 186.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 268.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.3,1000,3,0.0005,76.94,1.3
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.003 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.37 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 186.00 MiB memory in use. Process 13987 has 186.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 268.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.2,1000,3,0.001,79.46,4.71
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.003 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 186.00 MiB memory in use. Process 13987 has 186.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 268.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 258.00 MiB memory in use. Process 19549 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.2,1000,3,0.001,79.46,4.71
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.003 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 186.00 MiB memory in use. Process 13987 has 186.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 268.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 258.00 MiB memory in use. Process 19549 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.2,1000,3,0.001,79.46,4.71
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.003 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 186.00 MiB memory in use. Process 13987 has 186.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 268.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 258.00 MiB memory in use. Process 19549 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.2,1000,3,0.001,79.46,4.71
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.003 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 186.00 MiB memory in use. Process 13987 has 186.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 268.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 258.00 MiB memory in use. Process 19549 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.2,1000,3,0.001,79.46,4.71
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 3 hops and 0.003 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 186.00 MiB memory in use. Process 13987 has 186.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 268.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 258.00 MiB memory in use. Process 19549 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.2,1000,3,0.001,79.46,4.71
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.01 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 186.00 MiB memory in use. Process 13987 has 186.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 268.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 258.00 MiB memory in use. Process 19549 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.2,1000,3,0.001,79.46,4.71
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.01 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.37 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 186.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 16158 has 268.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 258.00 MiB memory in use. Process 19549 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.2,1000,3,0.001,84.59,3.21
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.01 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.46 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 186.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 258.00 MiB memory in use. Process 19549 has 180.00 MiB memory in use. Process 20099 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.0005,85.43,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.01 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 186.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 260.00 MiB memory in use. Process 19549 has 180.00 MiB memory in use. Process 20099 has 178.00 MiB memory in use. Process 20225 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.0005,85.43,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.01 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 186.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 260.00 MiB memory in use. Process 19549 has 182.00 MiB memory in use. Process 20099 has 178.00 MiB memory in use. Process 20225 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.0005,85.43,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.01 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 13439 has 186.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 260.00 MiB memory in use. Process 19549 has 182.00 MiB memory in use. Process 20099 has 178.00 MiB memory in use. Process 20225 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.01 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.22 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 260.00 MiB memory in use. Process 19549 has 182.00 MiB memory in use. Process 20099 has 178.00 MiB memory in use. Process 20225 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Process 20537 has 170.00 MiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.01 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 260.00 MiB memory in use. Process 19549 has 182.00 MiB memory in use. Process 20099 has 178.00 MiB memory in use. Process 20225 has 260.00 MiB memory in use. Process 20537 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.03 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 260.00 MiB memory in use. Process 19549 has 182.00 MiB memory in use. Process 20099 has 178.00 MiB memory in use. Process 20225 has 262.00 MiB memory in use. Process 20537 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.03 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 9472 has 178.00 MiB memory in use. Process 10712 has 180.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 264.00 MiB memory in use. Process 19549 has 182.00 MiB memory in use. Process 20099 has 178.00 MiB memory in use. Process 20225 has 262.00 MiB memory in use. Process 20537 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.03 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.55 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 264.00 MiB memory in use. Process 19549 has 182.00 MiB memory in use. Process 20099 has 180.00 MiB memory in use. Process 20225 has 262.00 MiB memory in use. Process 20537 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.03 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 264.00 MiB memory in use. Process 19549 has 182.00 MiB memory in use. Process 20099 has 180.00 MiB memory in use. Process 20225 has 262.00 MiB memory in use. Process 20537 has 178.00 MiB memory in use. Process 20941 has 178.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.03 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 264.00 MiB memory in use. Process 19549 has 182.00 MiB memory in use. Process 20099 has 180.00 MiB memory in use. Process 20225 has 262.00 MiB memory in use. Process 20537 has 178.00 MiB memory in use. Process 20941 has 178.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.03 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 264.00 MiB memory in use. Process 19549 has 182.00 MiB memory in use. Process 20099 has 180.00 MiB memory in use. Process 20225 has 262.00 MiB memory in use. Process 20537 has 180.00 MiB memory in use. Process 20941 has 178.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.03 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 266.00 MiB memory in use. Process 19549 has 182.00 MiB memory in use. Process 20099 has 180.00 MiB memory in use. Process 20225 has 262.00 MiB memory in use. Process 20537 has 180.00 MiB memory in use. Process 20941 has 178.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.03 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 266.00 MiB memory in use. Process 19549 has 182.00 MiB memory in use. Process 20099 has 180.00 MiB memory in use. Process 20225 has 262.00 MiB memory in use. Process 20537 has 180.00 MiB memory in use. Process 20941 has 178.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.001 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 266.00 MiB memory in use. Process 19549 has 182.00 MiB memory in use. Process 20099 has 180.00 MiB memory in use. Process 20225 has 262.00 MiB memory in use. Process 20537 has 180.00 MiB memory in use. Process 20941 has 178.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.001 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 266.00 MiB memory in use. Process 19549 has 182.00 MiB memory in use. Process 20099 has 180.00 MiB memory in use. Process 20225 has 262.00 MiB memory in use. Process 20537 has 180.00 MiB memory in use. Process 20941 has 178.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.001 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 266.00 MiB memory in use. Process 19549 has 182.00 MiB memory in use. Process 20099 has 180.00 MiB memory in use. Process 20225 has 262.00 MiB memory in use. Process 20537 has 180.00 MiB memory in use. Process 20941 has 178.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.001 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 266.00 MiB memory in use. Process 19549 has 182.00 MiB memory in use. Process 20099 has 180.00 MiB memory in use. Process 20225 has 262.00 MiB memory in use. Process 20537 has 180.00 MiB memory in use. Process 20941 has 178.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.001 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 266.00 MiB memory in use. Process 19549 has 182.00 MiB memory in use. Process 20099 has 180.00 MiB memory in use. Process 20225 has 262.00 MiB memory in use. Process 20537 has 180.00 MiB memory in use. Process 20941 has 178.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.001 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 266.00 MiB memory in use. Process 19549 has 182.00 MiB memory in use. Process 20099 has 180.00 MiB memory in use. Process 20225 has 262.00 MiB memory in use. Process 20537 has 180.00 MiB memory in use. Process 20941 has 178.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.001 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 266.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 180.00 MiB memory in use. Process 20225 has 262.00 MiB memory in use. Process 20537 has 180.00 MiB memory in use. Process 20941 has 178.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.001 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 268.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 180.00 MiB memory in use. Process 20225 has 264.00 MiB memory in use. Process 20537 has 180.00 MiB memory in use. Process 20941 has 178.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.003 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 268.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 180.00 MiB memory in use. Process 20225 has 264.00 MiB memory in use. Process 20537 has 180.00 MiB memory in use. Process 20941 has 178.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.003 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 268.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 180.00 MiB memory in use. Process 20225 has 264.00 MiB memory in use. Process 20537 has 180.00 MiB memory in use. Process 20941 has 178.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.003 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 268.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 180.00 MiB memory in use. Process 20225 has 264.00 MiB memory in use. Process 20537 has 180.00 MiB memory in use. Process 20941 has 178.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.003 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 268.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 264.00 MiB memory in use. Process 20537 has 180.00 MiB memory in use. Process 20941 has 178.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.003 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 270.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 264.00 MiB memory in use. Process 20537 has 180.00 MiB memory in use. Process 20941 has 178.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.003 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 270.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 264.00 MiB memory in use. Process 20537 has 180.00 MiB memory in use. Process 20941 has 178.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.003 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 270.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 264.00 MiB memory in use. Process 20537 has 182.00 MiB memory in use. Process 20941 has 178.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 4 hops and 0.003 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 270.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 264.00 MiB memory in use. Process 20537 has 182.00 MiB memory in use. Process 20941 has 178.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.01 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 270.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 264.00 MiB memory in use. Process 20537 has 182.00 MiB memory in use. Process 20941 has 178.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.01 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 11798 has 178.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 274.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 266.00 MiB memory in use. Process 20537 has 182.00 MiB memory in use. Process 20941 has 178.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.01 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.35 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 274.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 266.00 MiB memory in use. Process 20537 has 182.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.01 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 274.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 266.00 MiB memory in use. Process 20537 has 182.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.01 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 274.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 266.00 MiB memory in use. Process 20537 has 182.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.01 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 274.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 266.00 MiB memory in use. Process 20537 has 182.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.01 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 274.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 266.00 MiB memory in use. Process 20537 has 182.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.01 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 274.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 266.00 MiB memory in use. Process 20537 has 182.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.03 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 274.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 266.00 MiB memory in use. Process 20537 has 182.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.03 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 274.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 266.00 MiB memory in use. Process 20537 has 182.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.03 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19423 has 274.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 266.00 MiB memory in use. Process 20537 has 182.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.2,1000,3,0.001,91.96,2.55
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.03 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.45 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 266.00 MiB memory in use. Process 20537 has 182.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.3,1000,3,0.001,77.11,1.3
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.03 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 266.00 MiB memory in use. Process 20537 has 182.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.3,1000,3,0.001,77.11,1.3
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.03 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 266.00 MiB memory in use. Process 20537 has 182.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.3,1000,3,0.001,77.11,1.3
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.03 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 266.00 MiB memory in use. Process 20537 has 182.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.3,1000,3,0.001,77.11,1.3
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.03 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 268.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.3,1000,3,0.001,77.11,1.3
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.001 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 268.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.3,1000,3,0.001,77.11,1.3
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.001 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 184.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 268.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.3,1000,3,0.001,77.11,1.3
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.001 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 186.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 268.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.3,1000,3,0.001,77.11,1.3
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.001 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 186.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20225 has 268.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.3,1000,3,0.001,77.11,1.3
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.001 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.28 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 186.00 MiB memory in use. Process 20099 has 182.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Process 24126 has 176.00 MiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.001 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 186.00 MiB memory in use. Process 20099 has 184.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 260.00 MiB memory in use. Process 24126 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.001 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 186.00 MiB memory in use. Process 20099 has 184.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 260.00 MiB memory in use. Process 24126 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.001 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 186.00 MiB memory in use. Process 20099 has 184.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 260.00 MiB memory in use. Process 24126 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.003 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 186.00 MiB memory in use. Process 20099 has 184.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 260.00 MiB memory in use. Process 24126 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.003 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 186.00 MiB memory in use. Process 20099 has 184.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 260.00 MiB memory in use. Process 24126 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.003 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 186.00 MiB memory in use. Process 20099 has 184.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 264.00 MiB memory in use. Process 24126 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.003 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 186.00 MiB memory in use. Process 20099 has 184.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 264.00 MiB memory in use. Process 24126 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.003 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 186.00 MiB memory in use. Process 20099 has 184.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 264.00 MiB memory in use. Process 24126 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.003 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 186.00 MiB memory in use. Process 20099 has 184.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 264.00 MiB memory in use. Process 24126 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.003 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 186.00 MiB memory in use. Process 20099 has 184.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 264.00 MiB memory in use. Process 24126 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 5 hops and 0.003 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 186.00 MiB memory in use. Process 20099 has 184.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 264.00 MiB memory in use. Process 24126 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.01 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 186.00 MiB memory in use. Process 20099 has 184.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 264.00 MiB memory in use. Process 24126 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.01 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 186.00 MiB memory in use. Process 20099 has 184.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 264.00 MiB memory in use. Process 24126 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.01 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 186.00 MiB memory in use. Process 20099 has 184.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 264.00 MiB memory in use. Process 24126 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.01 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 186.00 MiB memory in use. Process 20099 has 184.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 266.00 MiB memory in use. Process 24126 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.01 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 188.00 MiB memory in use. Process 20099 has 184.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 266.00 MiB memory in use. Process 24126 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.01 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 188.00 MiB memory in use. Process 20099 has 184.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 266.00 MiB memory in use. Process 24126 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.01 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 188.00 MiB memory in use. Process 20099 has 186.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 266.00 MiB memory in use. Process 24126 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.01 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 188.00 MiB memory in use. Process 20099 has 186.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 266.00 MiB memory in use. Process 24126 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.03 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 188.00 MiB memory in use. Process 20099 has 186.00 MiB memory in use. Process 20537 has 184.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 268.00 MiB memory in use. Process 24126 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.03 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 188.00 MiB memory in use. Process 20099 has 186.00 MiB memory in use. Process 20537 has 186.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 268.00 MiB memory in use. Process 24126 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.03 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 188.00 MiB memory in use. Process 20099 has 186.00 MiB memory in use. Process 20537 has 186.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 268.00 MiB memory in use. Process 24126 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.03 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 188.00 MiB memory in use. Process 20099 has 186.00 MiB memory in use. Process 20537 has 186.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 268.00 MiB memory in use. Process 24126 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.03 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 188.00 MiB memory in use. Process 20099 has 186.00 MiB memory in use. Process 20537 has 186.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 268.00 MiB memory in use. Process 24126 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.03 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 188.00 MiB memory in use. Process 20099 has 186.00 MiB memory in use. Process 20537 has 186.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 270.00 MiB memory in use. Process 24126 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.03 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 188.00 MiB memory in use. Process 20099 has 186.00 MiB memory in use. Process 20537 has 186.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 270.00 MiB memory in use. Process 24126 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.03 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 188.00 MiB memory in use. Process 20099 has 186.00 MiB memory in use. Process 20537 has 186.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 270.00 MiB memory in use. Process 24126 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.001 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.17 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 188.00 MiB memory in use. Process 20099 has 186.00 MiB memory in use. Process 20537 has 186.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 270.00 MiB memory in use. Process 24126 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.001 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.17 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 188.00 MiB memory in use. Process 20099 has 186.00 MiB memory in use. Process 20537 has 186.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 272.00 MiB memory in use. Process 24126 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.001 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.17 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 19549 has 188.00 MiB memory in use. Process 20099 has 186.00 MiB memory in use. Process 20537 has 186.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 272.00 MiB memory in use. Process 24126 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.3,1000,3,0.001,85.45,1.44
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.001 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.36 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20099 has 186.00 MiB memory in use. Process 20537 has 186.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 272.00 MiB memory in use. Process 24126 has 264.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.3,1000,3,0.0005,80.54,5.9
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.001 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20099 has 186.00 MiB memory in use. Process 20537 has 186.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 272.00 MiB memory in use. Process 24126 has 264.00 MiB memory in use. Process 26384 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.3,1000,3,0.0005,80.54,5.9
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.001 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20099 has 186.00 MiB memory in use. Process 20537 has 186.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 272.00 MiB memory in use. Process 24126 has 264.00 MiB memory in use. Process 26384 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.3,1000,3,0.0005,80.54,5.9
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.001 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.36 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20537 has 186.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 274.00 MiB memory in use. Process 24126 has 264.00 MiB memory in use. Process 26384 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.3,1000,3,0.0005,82.16,1.79
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.001 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20537 has 186.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 274.00 MiB memory in use. Process 24126 has 264.00 MiB memory in use. Process 26384 has 180.00 MiB memory in use. Process 26682 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.3,1000,3,0.0005,82.16,1.79
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.003 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20537 has 186.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 274.00 MiB memory in use. Process 24126 has 266.00 MiB memory in use. Process 26384 has 180.00 MiB memory in use. Process 26682 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.3,1000,3,0.0005,82.16,1.79
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.003 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20537 has 186.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 23478 has 274.00 MiB memory in use. Process 24126 has 266.00 MiB memory in use. Process 26384 has 180.00 MiB memory in use. Process 26682 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.3,1000,3,0.0005,82.16,1.79
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.003 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.45 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20537 has 186.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 24126 has 266.00 MiB memory in use. Process 26384 has 180.00 MiB memory in use. Process 26682 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.0005,90.59,3.01
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.003 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.23 GiB is free. Process 6507 has 658.00 MiB memory in use. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 24126 has 266.00 MiB memory in use. Process 26384 has 180.00 MiB memory in use. Process 26682 has 178.00 MiB memory in use. Process 27019 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Process 27049 has 158.00 MiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.0005,90.59,3.01
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.003 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.85 GiB is free. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 24126 has 266.00 MiB memory in use. Process 26384 has 180.00 MiB memory in use. Process 26682 has 178.00 MiB memory in use. Process 27019 has 258.00 MiB memory in use. Process 27049 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.0005,90.59,3.01
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.003 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.53 GiB is free. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 24126 has 266.00 MiB memory in use. Process 26384 has 182.00 MiB memory in use. Process 26682 has 178.00 MiB memory in use. Process 27019 has 258.00 MiB memory in use. Process 27049 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Process 27204 has 324.00 MiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.0005,90.59,3.01
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.003 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 24126 has 266.00 MiB memory in use. Process 26384 has 182.00 MiB memory in use. Process 26682 has 178.00 MiB memory in use. Process 27019 has 258.00 MiB memory in use. Process 27049 has 178.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.0005,90.59,3.01
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 64 hidden channels and 10 hops and 0.003 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 24126 has 266.00 MiB memory in use. Process 26384 has 182.00 MiB memory in use. Process 26682 has 178.00 MiB memory in use. Process 27019 has 258.00 MiB memory in use. Process 27049 has 178.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.0005,90.59,3.01
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.01 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 24126 has 266.00 MiB memory in use. Process 26384 has 182.00 MiB memory in use. Process 26682 has 180.00 MiB memory in use. Process 27019 has 260.00 MiB memory in use. Process 27049 has 178.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.0005,90.59,3.01
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.01 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 24126 has 268.00 MiB memory in use. Process 26384 has 182.00 MiB memory in use. Process 26682 has 180.00 MiB memory in use. Process 27019 has 260.00 MiB memory in use. Process 27049 has 178.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.0005,90.59,3.01
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.01 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 24126 has 268.00 MiB memory in use. Process 26384 has 182.00 MiB memory in use. Process 26682 has 180.00 MiB memory in use. Process 27019 has 260.00 MiB memory in use. Process 27049 has 178.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.0005,90.59,3.01
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.01 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 24126 has 268.00 MiB memory in use. Process 26384 has 182.00 MiB memory in use. Process 26682 has 180.00 MiB memory in use. Process 27019 has 260.00 MiB memory in use. Process 27049 has 178.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.0005,90.59,3.01
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.01 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 15302 has 406.00 MiB memory in use. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 24126 has 268.00 MiB memory in use. Process 26384 has 182.00 MiB memory in use. Process 26682 has 180.00 MiB memory in use. Process 27019 has 260.00 MiB memory in use. Process 27049 has 178.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.0005,90.59,3.01
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.01 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.59 GiB is free. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 24126 has 268.00 MiB memory in use. Process 26384 has 182.00 MiB memory in use. Process 26682 has 180.00 MiB memory in use. Process 27019 has 262.00 MiB memory in use. Process 27049 has 180.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.0005,90.59,3.01
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.01 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 24126 has 268.00 MiB memory in use. Process 26384 has 182.00 MiB memory in use. Process 26682 has 180.00 MiB memory in use. Process 27019 has 262.00 MiB memory in use. Process 27049 has 180.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 404.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.0005,90.59,3.01
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.01 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 24126 has 268.00 MiB memory in use. Process 26384 has 182.00 MiB memory in use. Process 26682 has 180.00 MiB memory in use. Process 27019 has 262.00 MiB memory in use. Process 27049 has 180.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 404.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.0005,90.59,3.01
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.03 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 24126 has 268.00 MiB memory in use. Process 26384 has 182.00 MiB memory in use. Process 26682 has 180.00 MiB memory in use. Process 27019 has 262.00 MiB memory in use. Process 27049 has 180.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 404.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.0005,90.59,3.01
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.03 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 24126 has 268.00 MiB memory in use. Process 26384 has 182.00 MiB memory in use. Process 26682 has 180.00 MiB memory in use. Process 27019 has 262.00 MiB memory in use. Process 27049 has 180.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 404.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.0005,90.59,3.01
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.03 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 24126 has 268.00 MiB memory in use. Process 26384 has 182.00 MiB memory in use. Process 26682 has 180.00 MiB memory in use. Process 27019 has 264.00 MiB memory in use. Process 27049 has 180.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 404.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.0005,90.59,3.01
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.03 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 24126 has 268.00 MiB memory in use. Process 26384 has 182.00 MiB memory in use. Process 26682 has 180.00 MiB memory in use. Process 27019 has 264.00 MiB memory in use. Process 27049 has 180.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 404.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.0005,90.59,3.01
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.03 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 24126 has 268.00 MiB memory in use. Process 26384 has 184.00 MiB memory in use. Process 26682 has 180.00 MiB memory in use. Process 27019 has 264.00 MiB memory in use. Process 27049 has 180.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 404.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.0005,90.59,3.01
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.03 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.46 GiB is free. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 184.00 MiB memory in use. Process 26682 has 180.00 MiB memory in use. Process 27019 has 264.00 MiB memory in use. Process 27049 has 180.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 404.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.03 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 184.00 MiB memory in use. Process 26682 has 180.00 MiB memory in use. Process 27019 has 264.00 MiB memory in use. Process 27049 has 180.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 404.00 MiB memory in use. Process 28620 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.03 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 184.00 MiB memory in use. Process 26682 has 180.00 MiB memory in use. Process 27019 has 266.00 MiB memory in use. Process 27049 has 180.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 404.00 MiB memory in use. Process 28620 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.001 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 184.00 MiB memory in use. Process 26682 has 180.00 MiB memory in use. Process 27019 has 266.00 MiB memory in use. Process 27049 has 180.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 404.00 MiB memory in use. Process 28620 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.001 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 184.00 MiB memory in use. Process 26682 has 180.00 MiB memory in use. Process 27019 has 266.00 MiB memory in use. Process 27049 has 180.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 404.00 MiB memory in use. Process 28620 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.001 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.20 GiB is free. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 184.00 MiB memory in use. Process 26682 has 182.00 MiB memory in use. Process 27019 has 266.00 MiB memory in use. Process 27049 has 180.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 404.00 MiB memory in use. Process 28620 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.001 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 184.00 MiB memory in use. Process 26682 has 182.00 MiB memory in use. Process 27019 has 266.00 MiB memory in use. Process 27049 has 180.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 404.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.001 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 184.00 MiB memory in use. Process 26682 has 182.00 MiB memory in use. Process 27019 has 268.00 MiB memory in use. Process 27049 has 180.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 404.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.001 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 16061 has 306.00 MiB memory in use. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 184.00 MiB memory in use. Process 26682 has 182.00 MiB memory in use. Process 27019 has 268.00 MiB memory in use. Process 27049 has 180.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 404.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.001 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.49 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 184.00 MiB memory in use. Process 26682 has 182.00 MiB memory in use. Process 27019 has 268.00 MiB memory in use. Process 27049 has 182.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 404.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.001 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 184.00 MiB memory in use. Process 26682 has 182.00 MiB memory in use. Process 27019 has 268.00 MiB memory in use. Process 27049 has 182.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 404.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.003 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 184.00 MiB memory in use. Process 26682 has 182.00 MiB memory in use. Process 27019 has 268.00 MiB memory in use. Process 27049 has 182.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.003 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 184.00 MiB memory in use. Process 26682 has 182.00 MiB memory in use. Process 27019 has 268.00 MiB memory in use. Process 27049 has 182.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.003 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 184.00 MiB memory in use. Process 26682 has 182.00 MiB memory in use. Process 27019 has 268.00 MiB memory in use. Process 27049 has 182.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.003 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 184.00 MiB memory in use. Process 26682 has 182.00 MiB memory in use. Process 27019 has 268.00 MiB memory in use. Process 27049 has 182.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.003 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 184.00 MiB memory in use. Process 26682 has 182.00 MiB memory in use. Process 27019 has 268.00 MiB memory in use. Process 27049 has 182.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.003 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 184.00 MiB memory in use. Process 26682 has 182.00 MiB memory in use. Process 27019 has 268.00 MiB memory in use. Process 27049 has 182.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.003 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 184.00 MiB memory in use. Process 26682 has 182.00 MiB memory in use. Process 27019 has 270.00 MiB memory in use. Process 27049 has 182.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 3 hops and 0.003 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  2
32454656 out of  36052480
Computing k:  2  of  2
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 184.00 MiB memory in use. Process 26682 has 182.00 MiB memory in use. Process 27019 has 270.00 MiB memory in use. Process 27049 has 182.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.01 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 184.00 MiB memory in use. Process 26682 has 182.00 MiB memory in use. Process 27019 has 270.00 MiB memory in use. Process 27049 has 182.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.01 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 184.00 MiB memory in use. Process 26682 has 182.00 MiB memory in use. Process 27019 has 270.00 MiB memory in use. Process 27049 has 182.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.01 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.19 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 184.00 MiB memory in use. Process 26682 has 182.00 MiB memory in use. Process 27019 has 270.00 MiB memory in use. Process 27049 has 182.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.01 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.18 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 186.00 MiB memory in use. Process 26682 has 182.00 MiB memory in use. Process 27019 has 272.00 MiB memory in use. Process 27049 has 182.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.01 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.22 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 186.00 MiB memory in use. Process 26682 has 182.00 MiB memory in use. Process 27019 has 272.00 MiB memory in use. Process 27049 has 182.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.01 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.22 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 186.00 MiB memory in use. Process 26682 has 182.00 MiB memory in use. Process 27019 has 272.00 MiB memory in use. Process 27049 has 182.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.01 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.22 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 186.00 MiB memory in use. Process 26682 has 182.00 MiB memory in use. Process 27019 has 272.00 MiB memory in use. Process 27049 has 182.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.01 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.22 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 186.00 MiB memory in use. Process 26682 has 182.00 MiB memory in use. Process 27019 has 272.00 MiB memory in use. Process 27049 has 182.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.03 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 186.00 MiB memory in use. Process 26682 has 182.00 MiB memory in use. Process 27019 has 274.00 MiB memory in use. Process 27049 has 184.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.03 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 186.00 MiB memory in use. Process 26682 has 182.00 MiB memory in use. Process 27019 has 274.00 MiB memory in use. Process 27049 has 184.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.03 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 186.00 MiB memory in use. Process 26682 has 184.00 MiB memory in use. Process 27019 has 274.00 MiB memory in use. Process 27049 has 184.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.03 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 186.00 MiB memory in use. Process 26682 has 184.00 MiB memory in use. Process 27019 has 274.00 MiB memory in use. Process 27049 has 184.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.03 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.21 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 186.00 MiB memory in use. Process 26682 has 184.00 MiB memory in use. Process 27019 has 274.00 MiB memory in use. Process 27049 has 184.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 262.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.0005,85.9,1.15
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.03 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.48 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 186.00 MiB memory in use. Process 26682 has 184.00 MiB memory in use. Process 27049 has 184.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 264.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.4,1000,3,0.001,77.02,1.32
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.03 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.23 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 186.00 MiB memory in use. Process 26682 has 184.00 MiB memory in use. Process 27049 has 184.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 264.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.4,1000,3,0.001,77.02,1.32
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.03 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.23 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 186.00 MiB memory in use. Process 26682 has 184.00 MiB memory in use. Process 27049 has 184.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 264.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.4,1000,3,0.001,77.02,1.32
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.001 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.23 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 186.00 MiB memory in use. Process 26682 has 184.00 MiB memory in use. Process 27049 has 184.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 264.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.4,1000,3,0.001,77.02,1.32
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.001 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.23 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 188.00 MiB memory in use. Process 26682 has 184.00 MiB memory in use. Process 27049 has 184.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 264.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.4,1000,3,0.001,77.02,1.32
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.001 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.23 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 188.00 MiB memory in use. Process 26682 has 184.00 MiB memory in use. Process 27049 has 184.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 264.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.4,1000,3,0.001,77.02,1.32
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.001 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.23 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 188.00 MiB memory in use. Process 26682 has 184.00 MiB memory in use. Process 27049 has 184.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 264.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.4,1000,3,0.001,77.02,1.32
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.001 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.23 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 188.00 MiB memory in use. Process 26682 has 184.00 MiB memory in use. Process 27049 has 184.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 264.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.4,1000,3,0.001,77.02,1.32
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.001 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.23 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 188.00 MiB memory in use. Process 26682 has 184.00 MiB memory in use. Process 27049 has 184.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 264.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.4,1000,3,0.001,77.02,1.32
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.001 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.23 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 188.00 MiB memory in use. Process 26682 has 184.00 MiB memory in use. Process 27049 has 184.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 264.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.4,1000,3,0.001,77.02,1.32
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.001 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.23 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 188.00 MiB memory in use. Process 26682 has 184.00 MiB memory in use. Process 27049 has 184.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 264.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.4,1000,3,0.001,77.02,1.32
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.003 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.22 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 188.00 MiB memory in use. Process 26682 has 184.00 MiB memory in use. Process 27049 has 184.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 264.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.4,1000,3,0.001,77.02,1.32
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.003 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.22 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 188.00 MiB memory in use. Process 26682 has 184.00 MiB memory in use. Process 27049 has 184.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 264.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.4,1000,3,0.001,77.02,1.32
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.003 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.22 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20941 has 180.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 188.00 MiB memory in use. Process 26682 has 184.00 MiB memory in use. Process 27049 has 184.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 264.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.4,1000,3,0.001,77.02,1.32
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.003 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.40 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 188.00 MiB memory in use. Process 26682 has 186.00 MiB memory in use. Process 27049 has 184.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 266.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.4,1000,3,0.001,77.02,1.32
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.003 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.22 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 188.00 MiB memory in use. Process 26682 has 186.00 MiB memory in use. Process 27049 has 184.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 266.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 260.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.4,1000,3,0.001,77.02,1.32
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.003 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.22 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 188.00 MiB memory in use. Process 26682 has 186.00 MiB memory in use. Process 27049 has 186.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 266.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 264.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.4,1000,3,0.001,77.02,1.32
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.003 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.22 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 188.00 MiB memory in use. Process 26682 has 186.00 MiB memory in use. Process 27049 has 186.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 266.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 264.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.4,1000,3,0.001,77.02,1.32
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 4 hops and 0.003 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  3
32454656 out of  36052480
Computing k:  2  of  3
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.22 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 188.00 MiB memory in use. Process 26682 has 186.00 MiB memory in use. Process 27049 has 186.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 266.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 264.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.4,1000,3,0.001,77.02,1.32
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.01 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.22 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26384 has 188.00 MiB memory in use. Process 26682 has 186.00 MiB memory in use. Process 27049 has 186.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 266.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 264.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.4,1000,3,0.001,77.02,1.32
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.01 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.40 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26682 has 186.00 MiB memory in use. Process 27049 has 186.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 266.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 264.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.3,1000,3,0.001,78.11,6.22
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.01 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26682 has 186.00 MiB memory in use. Process 27049 has 186.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 266.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 264.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Process 32579 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.3,1000,3,0.001,78.11,6.22
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.01 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26682 has 186.00 MiB memory in use. Process 27049 has 186.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 266.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 264.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Process 32579 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.3,1000,3,0.001,78.11,6.22
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.01 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26682 has 186.00 MiB memory in use. Process 27049 has 186.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 266.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 264.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Process 32579 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.3,1000,3,0.001,78.11,6.22
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.01 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26682 has 186.00 MiB memory in use. Process 27049 has 186.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 266.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 264.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Process 32579 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.3,1000,3,0.001,78.11,6.22
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.01 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26682 has 186.00 MiB memory in use. Process 27049 has 186.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 266.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 264.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Process 32579 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.3,1000,3,0.001,78.11,6.22
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.01 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26682 has 186.00 MiB memory in use. Process 27049 has 186.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 266.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 266.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Process 32579 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.3,1000,3,0.001,78.11,6.22
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.03 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26682 has 186.00 MiB memory in use. Process 27049 has 186.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 266.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 266.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Process 32579 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.3,1000,3,0.001,78.11,6.22
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.03 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26682 has 186.00 MiB memory in use. Process 27049 has 186.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 266.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 266.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Process 32579 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.3,1000,3,0.001,78.11,6.22
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.03 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26682 has 186.00 MiB memory in use. Process 27049 has 186.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 266.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 266.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Process 32579 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.3,1000,3,0.001,78.11,6.22
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.03 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26682 has 186.00 MiB memory in use. Process 27049 has 186.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 266.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 266.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Process 32579 has 182.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.3,1000,3,0.001,78.11,6.22
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.03 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 26682 has 186.00 MiB memory in use. Process 27049 has 186.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 266.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 268.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Process 32579 has 182.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
texas,16,0.01,0.3,1000,3,0.001,78.11,6.22
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.03 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.42 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 27049 has 186.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 266.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 268.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Process 32579 has 182.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cornell,16,0.01,0.3,1000,3,0.001,82.97,2.11
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.03 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.25 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 20897 has 178.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 266.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 268.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Process 32579 has 182.00 MiB memory in use. Process 33491 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Process 33576 has 178.00 MiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.001,90.78,3.29
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.03 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.42 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 266.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 268.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Process 32579 has 182.00 MiB memory in use. Process 33491 has 178.00 MiB memory in use. Process 33576 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.001,90.78,3.29
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.001 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.25 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 266.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 268.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Process 32579 has 182.00 MiB memory in use. Process 33491 has 178.00 MiB memory in use. Process 33576 has 178.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.001,90.78,3.29
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.001 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 268.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 270.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Process 32579 has 182.00 MiB memory in use. Process 33491 has 178.00 MiB memory in use. Process 33576 has 178.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.001,90.78,3.29
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.001 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.25 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 268.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 270.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Process 32579 has 182.00 MiB memory in use. Process 33491 has 178.00 MiB memory in use. Process 33576 has 178.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.001,90.78,3.29
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.001 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.25 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 268.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 270.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Process 32579 has 182.00 MiB memory in use. Process 33491 has 178.00 MiB memory in use. Process 33576 has 178.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.001,90.78,3.29
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.001 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.25 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 22758 has 178.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 268.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 270.00 MiB memory in use. Process 32067 has 178.00 MiB memory in use. Process 32579 has 182.00 MiB memory in use. Process 33491 has 178.00 MiB memory in use. Process 33576 has 178.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.001,90.78,3.29
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.001 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.42 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 268.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 270.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 182.00 MiB memory in use. Process 33491 has 178.00 MiB memory in use. Process 33576 has 178.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wisconsin,16,0.01,0.3,1000,3,0.001,90.78,3.29
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.001 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 28620 has 268.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 272.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 182.00 MiB memory in use. Process 33491 has 178.00 MiB memory in use. Process 33576 has 180.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.001,86.1,1.14
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.001 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 272.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 182.00 MiB memory in use. Process 33491 has 180.00 MiB memory in use. Process 33576 has 180.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.001,86.1,1.14
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.003 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.25 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 272.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 182.00 MiB memory in use. Process 33491 has 180.00 MiB memory in use. Process 33576 has 180.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.001,86.1,1.14
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.003 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.25 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 272.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 182.00 MiB memory in use. Process 33491 has 180.00 MiB memory in use. Process 33576 has 180.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.001,86.1,1.14
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.003 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 274.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 182.00 MiB memory in use. Process 33491 has 180.00 MiB memory in use. Process 33576 has 180.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.001,86.1,1.14
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.003 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 274.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 182.00 MiB memory in use. Process 33491 has 180.00 MiB memory in use. Process 33576 has 180.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.001,86.1,1.14
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.003 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 274.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 182.00 MiB memory in use. Process 33491 has 180.00 MiB memory in use. Process 33576 has 180.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.001,86.1,1.14
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.003 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 274.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 180.00 MiB memory in use. Process 33576 has 180.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.001,86.1,1.14
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.003 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 31022 has 274.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 180.00 MiB memory in use. Process 33576 has 180.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
cora,16,0.01,0.4,1000,3,0.001,86.1,1.14
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 5 hops and 0.003 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  4
32454656 out of  36052480
Computing k:  2  of  4
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.51 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 180.00 MiB memory in use. Process 33576 has 180.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.01 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.25 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 180.00 MiB memory in use. Process 33576 has 180.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 262.00 MiB memory in use. Process 35065 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.01 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.25 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 180.00 MiB memory in use. Process 33576 has 180.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 262.00 MiB memory in use. Process 35065 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.01 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.25 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 180.00 MiB memory in use. Process 33576 has 180.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 262.00 MiB memory in use. Process 35065 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.01 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.25 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 180.00 MiB memory in use. Process 33576 has 180.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 262.00 MiB memory in use. Process 35065 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.01 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.25 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 180.00 MiB memory in use. Process 33576 has 180.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 262.00 MiB memory in use. Process 35065 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.01 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.25 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 180.00 MiB memory in use. Process 33576 has 180.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 262.00 MiB memory in use. Process 35065 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.01 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.25 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 180.00 MiB memory in use. Process 33576 has 180.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 262.00 MiB memory in use. Process 35065 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.01 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.25 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 182.00 MiB memory in use. Process 33576 has 182.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 262.00 MiB memory in use. Process 35065 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.03 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.25 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 182.00 MiB memory in use. Process 33576 has 182.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 262.00 MiB memory in use. Process 35065 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.03 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 182.00 MiB memory in use. Process 33576 has 182.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 262.00 MiB memory in use. Process 35065 has 260.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.03 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 182.00 MiB memory in use. Process 33576 has 182.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 262.00 MiB memory in use. Process 35065 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.03 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 182.00 MiB memory in use. Process 33576 has 182.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 262.00 MiB memory in use. Process 35065 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.03 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 18302 has 282.00 MiB memory in use. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 182.00 MiB memory in use. Process 33576 has 182.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 262.00 MiB memory in use. Process 35065 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.03 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.52 GiB is free. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 182.00 MiB memory in use. Process 33576 has 182.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 262.00 MiB memory in use. Process 35065 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.03 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.34 GiB is free. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 182.00 MiB memory in use. Process 33576 has 182.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 262.00 MiB memory in use. Process 35065 has 262.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Process 36076 has 184.00 MiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.03 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 182.00 MiB memory in use. Process 33576 has 182.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 262.00 MiB memory in use. Process 35065 has 264.00 MiB memory in use. Process 36076 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.001 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 182.00 MiB memory in use. Process 33576 has 182.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 264.00 MiB memory in use. Process 35065 has 264.00 MiB memory in use. Process 36076 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.001 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 182.00 MiB memory in use. Process 33576 has 182.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 264.00 MiB memory in use. Process 35065 has 264.00 MiB memory in use. Process 36076 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.001 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 182.00 MiB memory in use. Process 33576 has 182.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 264.00 MiB memory in use. Process 35065 has 264.00 MiB memory in use. Process 36076 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.001 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 182.00 MiB memory in use. Process 33576 has 182.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 264.00 MiB memory in use. Process 35065 has 266.00 MiB memory in use. Process 36076 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.001 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 182.00 MiB memory in use. Process 33576 has 182.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 264.00 MiB memory in use. Process 35065 has 266.00 MiB memory in use. Process 36076 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.001 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 182.00 MiB memory in use. Process 33576 has 182.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 264.00 MiB memory in use. Process 35065 has 266.00 MiB memory in use. Process 36076 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.001 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 182.00 MiB memory in use. Process 33576 has 182.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 264.00 MiB memory in use. Process 35065 has 266.00 MiB memory in use. Process 36076 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.001 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.24 GiB is free. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 184.00 MiB memory in use. Process 33491 has 182.00 MiB memory in use. Process 33576 has 182.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 264.00 MiB memory in use. Process 35065 has 266.00 MiB memory in use. Process 36076 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.003 learning rate and 0.2 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.23 GiB is free. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 186.00 MiB memory in use. Process 33491 has 182.00 MiB memory in use. Process 33576 has 184.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 264.00 MiB memory in use. Process 35065 has 268.00 MiB memory in use. Process 36076 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.003 learning rate and 0.2 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.23 GiB is free. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 186.00 MiB memory in use. Process 33491 has 182.00 MiB memory in use. Process 33576 has 184.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 264.00 MiB memory in use. Process 35065 has 268.00 MiB memory in use. Process 36076 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.003 learning rate and 0.3 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.23 GiB is free. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 186.00 MiB memory in use. Process 33491 has 182.00 MiB memory in use. Process 33576 has 184.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 266.00 MiB memory in use. Process 35065 has 268.00 MiB memory in use. Process 36076 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.003 learning rate and 0.3 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.23 GiB is free. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 186.00 MiB memory in use. Process 33491 has 182.00 MiB memory in use. Process 33576 has 184.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 266.00 MiB memory in use. Process 35065 has 268.00 MiB memory in use. Process 36076 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.003 learning rate and 0.4 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.23 GiB is free. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 186.00 MiB memory in use. Process 33491 has 182.00 MiB memory in use. Process 33576 has 184.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 266.00 MiB memory in use. Process 35065 has 268.00 MiB memory in use. Process 36076 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.003 learning rate and 0.4 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.23 GiB is free. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 186.00 MiB memory in use. Process 33491 has 184.00 MiB memory in use. Process 33576 has 184.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 266.00 MiB memory in use. Process 35065 has 268.00 MiB memory in use. Process 36076 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.003 learning rate and 0.5 dropout and 0.0005 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.28 GiB is free. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 186.00 MiB memory in use. Process 33491 has 184.00 MiB memory in use. Process 33576 has 184.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 266.00 MiB memory in use. Process 35065 has 268.00 MiB memory in use. Process 36076 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
Running squirrel with 128 hidden channels and 10 hops and 0.003 learning rate and 0.5 dropout and 0.001 weight decay
Computing the graphs...
Computing k:  1  of  9
32454656 out of  36052480
Computing k:  2  of  9
377501696 out of  4114199040
Traceback (most recent call last):
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/main.py", line 88, in <module>
    hops = khop_graphs_sparse(data.x,data.edge_index, args.hops,args.dataset,args.cuda,features=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ahmedbegga/Escritorio/git/CAEPIA2024_proceedings/utils.py", line 67, in khop_graphs_sparse
    A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.12 GiB. GPU 0 has a total capacty of 15.70 GiB of which 8.28 GiB is free. Process 27204 has 658.00 MiB memory in use. Process 27992 has 406.00 MiB memory in use. Process 29280 has 306.00 MiB memory in use. Process 32067 has 180.00 MiB memory in use. Process 32579 has 186.00 MiB memory in use. Process 33491 has 184.00 MiB memory in use. Process 33576 has 184.00 MiB memory in use. Process 33749 has 178.00 MiB memory in use. Process 34248 has 178.00 MiB memory in use. Process 34395 has 266.00 MiB memory in use. Process 35065 has 268.00 MiB memory in use. Process 36076 has 282.00 MiB memory in use. Including non-PyTorch memory, this process has 3.97 GiB memory in use. Of the allocated memory 496.12 MiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
citeseer,16,0.01,0.5,1000,3,0.0005,76.97,1.31
Best row:
texas,64,0.01,0.35,10000,4,0.0005,88.65,5.1
